{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will experiments with different variations of models, tokenizesr and other techniques\n",
    "\n",
    "\n",
    "MODEL_TRAINED_ON_ARABIC_TEXT_TO_FINE_TUNE:\n",
    "- google-bert/bert-base-multilingual-cased(and it variant) -> as a baseline\n",
    "- asafaya/bert-base-arabic\n",
    "- SI2M-Lab/DarijaBERT-arabizi\n",
    "- aubmindlab/bert-base-arabertv2\n",
    "- RoBERTa\n",
    "- DistilBERT(smaller and faster version)\n",
    "- ALBERT(Lite Bert)\n",
    "- ELECTRA\n",
    "\n",
    "The most of these models uses different technique for pre-training on large corpus, we would like to see the impact of those technique\n",
    "on our fine-tuned for sentiment analysis.\n",
    "\n",
    "TOKENIZERS:\n",
    "- Bert Embeddings itself\n",
    "- FastText(Best for handing OOV issue)\n",
    "- FastText + Bert Embedding(conctenated(context + semantic information combined) + initialize BERT embeddings with FastText's embeddings during training)\n",
    "- Glove + (Maybe the same thing)\n",
    "- Word2Vec\n",
    "\n",
    "Regularization Techniques:\n",
    "- Dropout\n",
    "- Weight decay\n",
    "- Layer normalization\n",
    "\n",
    "\n",
    "No extensive, I will add more when the base code source is implemented.\n",
    "\n",
    "#Multi-Task Learning\n",
    "#Ensemble Methods\n",
    "#....."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
