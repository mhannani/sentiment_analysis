{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT models trained on 80%/90% MYC dataset and evaluated on the the remaining set (20%/10%) MYC dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model_finetuned_on_mac_results_on_myc_80 = {\n",
    "    \"bert-base-multilingual-cased\": {\n",
    "        'checkpoint': \"bert-base-multilingual-cased/checkpoint-17185\",\n",
    "        \"eval_accuracy\": 0.8240223463687151,\n",
    "        \"f1\": 0.8233753984116612,\n",
    "        \"precision\": 0.8230600750938674,\n",
    "        \"recall\": 0.8238651215474427,\n",
    "    },\n",
    "    \"bert-base-arabic\": {\n",
    "        'checkpoint': \"bert-base-arabic/checkpoint-1414\",\n",
    "        \"accuracy\": 0.8355058969584109,\n",
    "        \"f1\": 0.8354561912202447,\n",
    "        \"precision\": 0.8372168211123873,\n",
    "        \"recall\": 0.8384746304202608,\n",
    "    },\n",
    "    \"darijabert-arabizi\": {\n",
    "        'checkpoint': \"darijabert-arabizi/checkpoint-1010\",\n",
    "        \"eval_accuracy\": 0.8578522656734947,\n",
    "        \"eval_f1\": 0.8575985535938784,\n",
    "        \"eval_loss\": 0.7308529019355774,\n",
    "        \"eval_precision\": 0.8574357464457305,\n",
    "        \"eval_recall\": 0.8591676105834858,\n",
    "    },\n",
    "    \"DarijaBERT\": {\n",
    "        'checkpoint': \"DarijaBERT/checkpoint-404\",\n",
    "        \"eval_accuracy\": 0.8504034761018001,\n",
    "        \"eval_f1\": 0.8502025587999259,\n",
    "        \"eval_loss\": 0.3632583022117615,\n",
    "        \"eval_precision\": 0.8503177853855328,\n",
    "        \"eval_recall\": 0.8520786500537306,\n",
    "    },\n",
    "    \"bert-base-arabertv2\": {\n",
    "        'checkpoint': \"bert-base-arabertv2/checkpoint-5656\",\n",
    "        \"eval_accuracy\": 0.8485412787088765,\n",
    "        \"f1\": 0.8481283794356451,\n",
    "        \"loss\": 1.2586467266082764,\n",
    "        \"precision\": 0.8477460119078617,\n",
    "        \"recall\": 0.8491034242397839,\n",
    "        }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model_finetuned_on_mac_results_on_myc_90 = {\n",
    "    \"bert-base-multilingual-cased\": {\n",
    "        'checkpoint': \"bert-base-multilingual-cased/checkpoint-1816\",\n",
    "        \"eaccuracy\": 0.8249534450651769,\n",
    "        \"f1\": 0.8249236958934517,\n",
    "        \"precision\": 0.8272427096299911,\n",
    "        \"recall\": 0.8282434115540244,\n",
    "    },\n",
    "    \"bert-base-arabic\": {\n",
    "        'checkpoint': \"bert-base-arabic/checkpoint-681\",\n",
    "        \"eval_accuracy\": 0.8355058969584109,\n",
    "        \"eval_f1\": 0.8347632832067104,\n",
    "        \"eval_loss\": 0.5109553933143616,\n",
    "        \"eval_precision\": 0.8345833075447175,\n",
    "        \"eval_recall\": 0.8349751519695627,\n",
    "    },\n",
    "    \n",
    "    \"darijabert-arabizi\": {\n",
    "        'checkpoint': \"darijabert-arabizi/checkpoint-7945\",\n",
    "        \"accuracy\": 0.851024208566108,\n",
    "        \"f1\": 0.8506830947187602,\n",
    "        \"loss\": 1.3694322109222412,\n",
    "        \"precision\": 0.8503654869303724,\n",
    "        \"recall\": 0.8519536066823232,\n",
    "    },\n",
    "    \"DarijaBERT\": {\n",
    "        'checkpoint': \"DarijaBERT/checkpoint-14239\",\n",
    "        'accuracy': 0.6189618775611573, \n",
    "        'precision': 0.6813790067106706, \n",
    "        'recall': 0.637299477516956, \n",
    "        'f1': 0.600870422702932\n",
    "    },\n",
    "    \"bert-base-arabertv2\": {\n",
    "        'checkpoint': \"bert-base-arabertv2/checkpoint-8347\",\n",
    "        'accuracy': 0.560163914069291, \n",
    "        'precision': 0.5944748608070536, \n",
    "        'recall': 0.576292962461424, \n",
    "        'f1': 0.5443771407643445\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# adding the preprocessing \n",
    "# double check the scores of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'exp_name': 'experiment-train_on_myc_80', 'model_id': 'bert-base-multilingual-cased', 'epoch': 4.0, 'eval_accuracy': 0.8280571073867163, 'eval_f1': 0.8277129742558018, 'eval_loss': 0.6178282499313354, 'eval_precision': 0.8275118032115836, 'eval_recall': 0.8290459179228021, 'eval_runtime': 25.5392, 'eval_samples_per_second': 126.159, 'eval_steps_per_second': 1.997, 'step': 808} \n",
    "\n",
    "{'exp_name': 'experiment-train_on_myc_80', 'model_id': 'bert-base-arabic', 'epoch': 7.0, 'eval_accuracy': 0.8355058969584109, 'eval_f1': 0.8354561912202447, 'eval_loss': 0.9907687306404114, 'eval_precision': 0.8372168211123873, 'eval_recall': 0.8384746304202608, 'eval_runtime': 25.5142, 'eval_samples_per_second': 126.282, 'eval_steps_per_second': 1.999, 'step': 1414} \n",
    "\n",
    "{'exp_name': 'experiment-train_on_myc_80', 'model_id': 'darijabert-arabizi', 'epoch': 5.0, 'eval_accuracy': 0.8578522656734947, 'eval_f1': 0.8575985535938784, 'eval_loss': 0.7308529019355774, 'eval_precision': 0.8574357464457305, 'eval_recall': 0.8591676105834858, 'eval_runtime': 25.4397, 'eval_samples_per_second': 126.653, 'eval_steps_per_second': 2.005, 'step': 1010} \n",
    "\n",
    "{'exp_name': 'experiment-train_on_myc_80', 'model_id': 'DarijaBERT', 'epoch': 2.0, 'eval_accuracy': 0.8504034761018001, 'eval_f1': 0.8502025587999259, 'eval_loss': 0.3632583022117615, 'eval_precision': 0.8503177853855328, 'eval_recall': 0.8520786500537306, 'eval_runtime': 25.5517, 'eval_samples_per_second': 126.097, 'eval_steps_per_second': 1.996, 'step': 404} \n",
    "\n",
    "{'exp_name': 'experiment-train_on_myc_80', 'model_id': 'bert-base-arabertv2', 'epoch': 22.0, 'eval_accuracy': 0.8376784605834885, 'eval_f1': 0.8371060328553381, 'eval_loss': 1.2528820037841797, 'eval_precision': 0.836756659478364, 'eval_recall': 0.8376782550608464, 'eval_runtime': 25.5635, 'eval_samples_per_second': 126.039, 'eval_steps_per_second': 1.995, 'step': 4444}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'exp_name': 'experiment-train_on_myc_90', 'model_id': 'bert-base-multilingual-cased', 'epoch': 8.0, 'eval_accuracy': 0.8249534450651769, 'eval_f1': 0.8249236958934517, 'eval_loss': 0.8050663471221924, 'eval_precision': 0.8272427096299911, 'eval_recall': 0.8282434115540244, 'eval_runtime': 13.1388, 'eval_samples_per_second': 122.614, 'eval_steps_per_second': 1.979, 'step': 1816} \n",
    "\n",
    "{'exp_name': 'experiment-train_on_myc_90', 'model_id': 'bert-base-arabic', 'epoch': 3.0, 'eval_accuracy': 0.8355058969584109, 'eval_f1': 0.8347632832067104, 'eval_loss': 0.5109553933143616, 'eval_precision': 0.8345833075447175, 'eval_recall': 0.8349751519695627, 'eval_runtime': 13.1142, 'eval_samples_per_second': 122.844, 'eval_steps_per_second': 1.983, 'step': 681} \n",
    "\n",
    "{'exp_name': 'experiment-train_on_myc_90', 'model_id': 'darijabert-arabizi', 'epoch': 35.0, 'eval_accuracy': 0.851024208566108, 'eval_f1': 0.8506830947187602, 'eval_loss': 1.3694322109222412, 'eval_precision': 0.8503654869303724, 'eval_recall': 0.8519536066823232, 'eval_runtime': 13.083, 'eval_samples_per_second': 123.137, 'eval_steps_per_second': 1.987, 'step': 7945} \n",
    "\n",
    "{'exp_name': 'experiment-train_on_myc_90', 'model_id': 'DarijaBERT', 'epoch': 3.0, 'eval_accuracy': 0.8423339540657977, 'eval_f1': 0.8416868107549313, 'eval_loss': 0.4947100281715393, 'eval_precision': 0.8414071680376027, 'eval_recall': 0.8420608683797969, 'eval_runtime': 13.1376, 'eval_samples_per_second': 122.625, 'eval_steps_per_second': 1.979, 'step': 681} \n",
    "\n",
    "{'exp_name': 'experiment-train_on_myc_90', 'model_id': 'bert-base-arabertv2', 'epoch': 6.0, 'eval_accuracy': 0.8342644320297952, 'eval_f1': 0.8340342189610688, 'eval_loss': 0.6955660581588745, 'eval_precision': 0.8341568745915687, 'eval_recall': 0.835865126625811, 'eval_runtime': 13.1194, 'eval_samples_per_second': 122.796, 'eval_steps_per_second': 1.982, 'step': 1362}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'exp_name': 'experiment-train_on_preprocessed_myc_80', 'model_id': 'bert-base-multilingual-cased', 'epoch': 30.0, 'eval_accuracy': 0.8317757009345794, 'eval_f1': 0.8312684632250131, 'eval_loss': 1.1656997203826904, 'eval_precision': 0.8308764540714, 'eval_recall': 0.8322857887850409, 'eval_runtime': 26.5634, 'eval_samples_per_second': 120.843, 'eval_steps_per_second': 1.92, 'step': 6030} \n",
    "\n",
    "{'exp_name': 'experiment-train_on_preprocessed_myc_80', 'model_id': 'bert-base-arabic', 'epoch': 3.0, 'eval_accuracy': 0.8305295950155763, 'eval_f1': 0.8304078664395907, 'eval_loss': 0.49232855439186096, 'eval_precision': 0.8314319268517743, 'eval_recall': 0.8331230697648275, 'eval_runtime': 26.5578, 'eval_samples_per_second': 120.869, 'eval_steps_per_second': 1.92, 'step': 603} \n",
    "\n",
    "{'exp_name': 'experiment-train_on_preprocessed_myc_80', 'model_id': 'darijabert-arabizi', 'epoch': 1.0, 'eval_accuracy': 0.856386292834891, 'eval_f1': 0.8556124452683831, 'eval_loss': 0.3315466046333313, 'eval_precision': 0.8555642794398902, 'eval_recall': 0.8556621272635863, 'eval_runtime': 26.4521, 'eval_samples_per_second': 121.351, 'eval_steps_per_second': 1.928, 'step': 201} \n",
    "\n",
    "{'exp_name': 'experiment-train_on_preprocessed_myc_80', 'model_id': 'DarijaBERT', 'epoch': 4.0, 'eval_accuracy': 0.8501557632398754, 'eval_f1': 0.8497964531374291, 'eval_loss': 0.5188102126121521, 'eval_precision': 0.8494739532374393, 'eval_recall': 0.8512087292690009, 'eval_runtime': 26.5855, 'eval_samples_per_second': 120.743, 'eval_steps_per_second': 1.918, 'step': 804} \n",
    "\n",
    "{'exp_name': 'experiment-train_on_preprocessed_myc_80', 'model_id': 'bert-base-arabertv2', 'epoch': 2.0, 'eval_accuracy': 0.8370716510903426, 'eval_f1': 0.8370544300107998, 'eval_loss': 0.40349188446998596, 'eval_precision': 0.8400343140251083, 'eval_recall': 0.840932227471872, 'eval_runtime': 26.5216, 'eval_samples_per_second': 121.033, 'eval_steps_per_second': 1.923, 'step': 402}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
